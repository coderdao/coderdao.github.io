---
layout:     post
title:      Linux 查询大日志文件
subtitle:   Linux 查询大日志文件
date:       2021-06-08
author:     锐玩道
header-img: img/bg_img/post-bg-cook.jpg
catalog:      true
theme:      smartblue
tags:
    - 面试
    - linux
---

# linux 查询大日志文件

> 如果❤️我的文章有帮助，欢迎点赞、关注。这是对我继续技术创作最大的鼓励。[更多系列文章在我博客](https://coderdao.github.io/)

  

## 场景

今天隔离还在继续，在家办公。忽然下午午工作群里发来一个 `mysql` 机器io/负载上升的预警，异常发生在 `15：45 ~ 16：00` 之间。为了事后为了查明原因，需要翻看慢查询日志 `slow.log` 才发现日志 8G 多... 故事就这样开始了

![图片描述](http://img1.sycdn.imooc.com/60be351400015bfd13080226.png)

  

怎么办呢。第一个想到的就是常用 `grep` 匹配关键字

## 思路 `grep 关键字`

grep 常用于 `关键字` 匹配文件文本信息。<br/>

但关键字从哪里来呢，可以命令 `head slow3306_9110.log` 查看下检索文件的`内容结构`

![图片描述](http://img.mukewang.com/60beb6350001859b16440835.png)

  

因为异常发生在 `15：45 ~ 16：00` 之间，我就可以这样写

> grep -n 'Time: 210607 15:[45-59]' slow3306_9110.log

  

时间 `15：45` 至 `15：59` 之间内容，但这样匹配只能看到时间，这明显不是我们想要的
![图片描述](http://img.mukewang.com/60beb6820001711d16440835.png)

  

### grep 显示匹配行附近内容
```text
A -> After

B -> Before

C -> Context
```
举个例子：

>  `grep -A5 'Time: 210607 15:[45-59]' slow3306_9110.log`

  

就能把匹配 `Time: 210607` 行的`下面 5 行`也显示出来。
![图片描述](http://img.mukewang.com/60beb71a00019bf116440844.png)

  
  

### grep 多关键字搜索

但这时我们有会发现，`Query_time: 0.925375` 查询时间有大有小。我现在在查故障明细是只想看 `查询消耗时间大的`。

  

所以这里就需要用到 `grep 多关键字搜索`

  

#### 匹配多个关键字（且）

管道符连接 `多个条件` 实现关键字 `且关系` 匹配：

> grep -A5 'Time: 210607 15:[45-59]' slow3306_9110.log | grep 'Query_time: (\d[2-5])'

  

同一行同时满足两个条件（`Time`、`Query_time`）才能够匹配。

  

不过这里也必须说明：`因为上图内容格式中，Time 和 Query_time 不在同一列`，所以上诉命令只是这个演示。实际只能匹配 `同一行同时满足两个条件` 内容

  

#### grep -E 匹配多个关键字（或）

  

> grep -E "word1|word2|word3" file.txt

  

匹配文件中 同一行包含 word1、word2、word3 之一

  

### `grep` 总结

总结下来。

由于多行无法同时命中 `时间 15：45 至 15：59` 和 `查询时间在 2~5位整数之间`。

另外由于文件太大，grep 一次就能跑个 3、4 分钟实际体验并不好

  

## 转变思路

前文已经总结过 `grep` 方法 `处理大文件的时候`：`grep 检索一次的时间消耗高`、`多行内容难以匹配多个关键字` 等困难。实在是难受啊，怎么办呢

  
  

用后端处理大量数据的思路：就是`把数据不断地细分，削峰填谷用时间换空间`。<br/>

简而言之：既然文件大，一次处理不方便。那就把文件平均切成好几份不断细分。一次处理一份。处理的次数多了，但平均`每次处理的数据量更小、时间更少、更迅速`。体验也更好

  

## 思路 `split 切分文件`

  

split 是 linux 常用的 `文件切分方法`。它支持按行、大小等方式进行切分。

  

split命令的语法如下：`split [-a ][-b][-C ][-l ][被切割文件路径][输出文件名前缀]`

  

具体参数如下：

![图片描述](http://img.mukewang.com/60bead2a0001604716440835.png)

  

### 切分大文件

  

下面举几个例子：

## 文件大小切割

将 `slow3306_9105.log` 文件 按文件大小（每个文件 10m）分割成多个文件

```shell

$ split -b 10m slow3306_9105.log

$ ls

slow3306_9105.log xaa xab xac xad xae

```

  

## 设定输出文件名

上面看起来 `xaa xab xac xad xae` 就是输出文件，但并不直观。<br/>

所以还能对输出文件名进行设定

```shell

$ split -d -b 10m slow3306_9105.log slow.log

$ ls

slow3306_9105.log slow.log00 slow.log01 slow.log02 slow.log03 slow.log04

```

  

## 文件行数切割

将 `slow3306_9105.log` 文件 按 行数（每个文件 5w 行）分割成多个文件

![图片描述](http://img.mukewang.com/60beaed600011d7516440835.png)

  

## 输出文件保持 `log` 文件后缀

  

生成后文件名都是 `.log0*` ，生成的后缀被追加到最后影响文件格式。而需要`.log`文件结尾的话，可以使用下面命令 `将分割后的文件序号调整至文件名后`，不影响文件后缀：

```bash

for  i  in  `ls|grep slow`; do a=`echo $i|awk -F '.log' '{print $1$2".log"}'`; mv $i  $a; done

```

![图片描述](http://img.mukewang.com/60beb0830001484e16440835.png)

  

## 后悔药：日志合并

如果你切着切着把文件且太细了（不是一般人）。那么附带记录一个 `多个文件合并的方法`：`cat 命令`将切分后文件 重新合并 为同一文件：`$ cat slow0* > original.log`

  
  

## 总结

切割成小文件之后，无论是下载到本地再处理、本地查看、还是条件检索都非常方便了